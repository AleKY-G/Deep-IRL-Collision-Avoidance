\documentclass[conference]{IEEEtran}
\include{docs/latex/commands}
\usepackage{todonotes}
\pagenumbering{arabic}
\pagestyle{plain}
\begin{document}

\title{AA277: Collision Avoidance Final Project\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Valentin Antoine}
\IEEEauthorblockA{\textit{Dept. of Mechanical Engineering} \\
\textit{Stanford University}\\
valent1@stanford.edu}
\and
\IEEEauthorblockN{Bradley Collicott}
\IEEEauthorblockA{\textit{Dept. of Aeronautics \& Astronautics} \\
\textit{Stanford University}\\
collicott@stanford.edu}
\linebreakand
\IEEEauthorblockN{Brian Dobkowski}
\IEEEauthorblockA{\textit{Dept. of Mechanical Engineering} \\
\textit{Stanford University}\\
bdobkows@stanford.edu}
\and
\IEEEauthorblockN{Torstein Eliassen}
\IEEEauthorblockA{\textit{Dept. of Electrical Engineering} \\
\textit{Stanford University}\\
torsteoe@stanford.edu}}

\maketitle
\begin{abstract}
Multi-robot systems have been widely applied to perform a given task collaboratively and cooperatively. A major challenge for these systems is to avoid collision, either between robots, or with other agents (e.g. human) as well as its environment.
Collision avoidance strategies traditionally leverage model-based algorithms with many tunable parameters. Recent advances in reinforcement learning have allowed for learning-based methods to supplement the state-of-the-art model-based collision avoidance algorithms. This work presents a decentralized multiagent collision avoidance algorithm based on deep reinforcement learning, which learns from a method-based algorithm named Optimal Reciprocal Collision Avoidance (ORCA).
Specifically, the proposed approach develops a value network that encodes the estimated time to the goal given an agent’s joint configuration (positions and velocities) with its neighbor, as a 2 agents system is considered for simplicity. This value network can then be used in one-step lookahead for policy generation.
The value of different parameters, functions, have been changed to see how the reinforcement learning algorithm reacts.
\end{abstract}
\section{Introduction}
Collision avoidance is a principle problem in mutlt-robot control. Many pathplanning methods in literature make broad assumptions about information sharing and connection to a centralized planner to ensure collision-free paths; however, such a system becomes unwieldy and unrealistic when dealing with many agents. Thus, the subject of interest for this project proposal is decentralized collision avoidance, in which multiple agents plan safe trajectories without communication with neighoring agents. The literature can be divided between model-based and learning-based approaches to collision avoidance, both of which will be surveyed here.
\section{Literature Review}
% At least 10 references: \begin{itemize}
%     \item Show depth of understanding in topic.
%     \item Explain how the references relate to one another.
%     \item Discuss the open research areas and unsolved problems in the topic area.
% \end{itemize}

\subsection{Model-Based Collision Avoidance}
The Dynamic Window Approach to Collision Avoidance \cite{fox1997dynamic} presents an approach for collision avoidance for single-robot systems. This is foundational work that takes into account the dynamics of the robot for creating collision-free paths. The method is in the group of local methods allowing for fast reaction to world changes. By solving an optimization problem the approach successfully weighs three objectives: "heading": i.e. progress towards goal, "clearance": distance to the closest obstacle and "velocity": prioritizing high velocities, constrained to collision-free velocities. 

Reciprocal n-Body Collision Avoidance \cite{berg2011reciprocal} discusses multiple-robot collision avoidance. The paper provides an approach to collision-free movement for each robot for a fixed duration, for robots using the same local protocol. The approach uses linear programming in order to find collision-free velocities. The term reciprocal collision avoidance refers to multiple robots attempting to avoid collisions simultaneously without communicating, but using the same strategy. 
The paper introduces ORCA: Optimal Reciprocal Collision Avoidance. The method deals with both intelligent dynamical objects and static objects. Kinematics and dynamic constraints are not taken into account in this paper.

This paper leverages the idea of Reciprocal Velocity Obstacles (RVO) to define constrained regions in the motion of each robot. A convex cone is developed to describe the relative velocity of robot $A$ with respect to robot $B$ over a time interval $\tau$. If the relative velocity is outside the cone for the window $\tau$, it can be guaranteed to be collision free for that time window. The ORCA formulation finds the optimal velocity of robot $A$ that avoids being in this constrained region, and is the closest to its "optimization velocity" or desired velocity. For the approach in this paper, no network between robots is required, but it is assumed the robots have perfect sensing capability (of their own states and other robot's states) and know the other robots' optimization velocities. Future directions involve incorporating robot motion constraints, sensing uncertainty, and more dimensions of motion (3D). Alonso et. al. \cite{alonso2013optimal} apply motion/dynamics constraints to extend the ORCA approach to nonholonomic robots.

Confidence Aware Motion Planning \cite{fridovich2020confidence} attempts to improve predictions of other intelligent agents. By introducing a confidence parameter, a Bayesian belief, over the predictions, the robot better reacts to unexpected behaviour.

Schwager et. al. in 2017 \cite{schwager2017} attempted to resolve some of the computational challenges associated with the RVO-based techniques (namely ORCA) by using buffered Voronoi regions to ensure collision avoidance between multiple robots. Borrowed from coverage control, these Voronoi regions are buffered to ensure that the whole of the robot geometry is encompassed within the cell at all times. Using this approach requires only position estimates of the other robots, thereby avoiding the need to obtain velocity estimates which are usually more corrupted by noise due to the practicality of the sensors used. This method is suitable for application in online settings because a solving method that leverages simple geometric details was developed to reduce the complexity incurred by a quadratic program at each time step. Another advantage of this method is that it does not require all robots to be running with the same algorithm.

%% Bradley
\subsection{Learning-Based Collision Avoidance}
To approximate the performance of centralized collision avoidance algorithms in a decentralized manner, several works have leveraged deep reinforcement learning to approximate the optimal policy for agents to cooperatively avoid collisions. Chen et. al. \cite{chen2017cadrl} formulate the collision avoidance problem as a sequential decision-making problem with partially observable agent states. Rather than using a model-based algorithm to plan control inputs, the authors use offline deep reinforcement learning to encode a learned value function into a neural network. This value function is embedded in each robot, and the optimal action at each observation time is selected by repeatedly maximizing the one-step lookahead value. This ensures that, on average, each robot behaves in a way that maximizes the value of the joint state. The authors pre-train the neural network using supervised learning with demonstration trajectory solutions from a state-of-the-art model-based collision avoidance algorithm, followed by reinforcement learning episodes with randomized environments. The resulting learned value function from this method, termed CADRL, leads to a policy that significantly outperforms the ORCA algorithm in terms of path quality.

Long et. al. \cite{long2018} also formulate the collision avoidance problem as a partially observable sequential decision-making problem; however, in contrast to the CADRL policy that maps ego and neighbor agent state information to control decisions, the authors of \cite{long2018} attempt to learn a policy that maps raw sensor information to a control decision. The purpose of crafting the policy in this manner is to reduce the necessary complexity of the perception stack as compared with a policy that accepts derived quantities as inputs and to explicitly account for sensing uncertainty in the learned collision avoidance policy. The authors eschew a supervised learning step in their algorithm, and instead opt for a policy-gradient-based reinforcement learning algorithm that is trained on a set of curated environments with varying number of agents. Using 2D LiDAR-like range measurements as the input to the learned policy, this sensor-level reinforcement learning process yields a policy that outperforms both the ORCA algorithm and a supervised-learning-based policy in terms of success rate and time-to-goal. The authors demonstrate several scenarios unseen in training to which the agents adapted.

While the aforementioned reinforcement learning strategies demonstrated great success, they did not explicitly encode agent cooperation into their learned value networks. Sartoretti et. al. \cite{sartoretti2019} attempt to impart this behavior into decentralized collision avoidance by crafting a reward function that penalizes actions that hinder other agents in the environment. In addition, the authors use a joint reinforcement learning and imitation learning strategy, similar to \cite{chen2017cadrl}, that randomly introduces episodes of ‘expert’ trajectory demonstrations into the learning process. The problem is formulated as a partially observable sequential decision-making problem, where the partial-observability is introduced by limiting the field-of-view that each robot can see in the environment. This work, although promising in terms of scalability and collaborative collision avoidance and trajectory planning, is limited in scope due to its discrete state and action space, whereas the previously surveyed deep reinforcement learning collision avoidance works operated on continuous state and action spaces.

Kahn et. al. \cite{kahn2017} attempt to address the problem of online reinforcement learning for collision avoidance. By recognizing that agents must experience collisions to learn how to avoid them, they formulate an uncertainty-aware reinforcement process that uses a velocity-dependent collision cost function in tandem with uncertainty-aware collision estimates that results in agents navigating uncertain environments, and thereby experiencing collisions, at significantly lower speeds than traditional reinforcement learning methods.
%%

%% Valentin
Socially Aware Motion Planning with Deep Reinforcement Learning \cite{SocialCA} aims at enhancing the behaviour of a robot in an environment filled with humans, taking into account the stochasticity in people's behaviour. A learning-based approach is adopted - as opposed to a model-based approach - in order to reach socially aware collision avoidance by learning human-like navigation conventions. This was achieved with deep reinforcement learning for inducing socially aware behaviors in a reinforcement learning framework.

Crowd-Aware Robot Navigation With Attention-Based Deep Reinforcement Learning \cite{Crowd} underlines the effectiveness of reinforcement frameworks\cite{SocialCA} to learn socially cooperative policies, but shows that such approach needs to be enhanced for a crowd as these cooperative policies assume a one-way Human-Robot interaction problem. Such approach is enhanced by using pairwise interactions between the robot and each human and by capturing the interactions among humans via local maps. 

Model-based approaches differ from learning-based approach as they usually use additional parameters to account for social interactions with humans, adding that to usual multi-agent collision avoidance algorithms. Such approach is proposed in \cite{Ferrer}, quoted by \cite{SocialCA}, where they are the first to use Social Force Model in order to represent the social interactions. The idea is that changes in trajectories can be explained in terms of social fields or forces, and these social fields depend upon the nature of the agents interacting.
%%

\section{Problem Approach}
Given that the state-of-the-art model-based methods in collision avoidance have now been eclipsed by reinforcement-learning-based methods, this project will focus on reproducing a foundational work in collision avoidance using deep reinforcement learning (CADRL). A general challenge in using learning policies on real robots is the question of safety and reliability. The intent of the project is to reproduce the learned value network from \cite{chen2017cadrl} and perform a parameter / ablation study using the hyperparameters of the algorithm. The CADRL algorithm can be systematically interrogated by changing parameters like the reward function, robot dynamics, and state/sensing uncertainty to see how well the learned policy adapts to unseen conditions that were not investigated in the original work.

\subsection{Problem Formulation}
Decentralized, un-communicative collision avoidance will be posed as a partially-observable sequential decision-making problem. The pairwise joint state to be considered by the policy for ego-robot $i$ and neighbor robot $j$ is $\state{ij}{}=\begin{bmatrix} \state{i}{} & \state{j}{o} \end{bmatrix}\inR{14}$ where each robot state is comprised of observable $\state{}{o}$ and hidden $\state{}{h}$ quantities. The observable quantities are position and velocity $\posvec{}, \,  \velvec{} \inR{2}$ and robot body radius $\radius{} \inR{}$, and the hidden quantities are goal position $\posvec{g} \inR{2}$, preferred speed, and heading angle $\vel{pref}, \, \headangle{}\inR{}$.

The considered action space is continuous with a policy $\policy{}(\state{ij}{})=\ctrl{i} \, : \, \mathbb{R}^{15}\mapsto\mathbb{R}^2$.  During propagation, upper and lower bounds on the change in heading angle restrict the action space. By constraining the action space, we enforce reasonable commanded control input. Single-integrator dynamic will be used as the baseline model: 
\begin{alignat}{3}
& \state{i}{t+1} & \ = \ & \state{i}{t} + \ctrl{i}{t}\dt = \state{i}{t} + \policy{}(\state{ij}{})\dt \label{eqn:dyn1} \\
& \state{j}{t+1} & \ = \ &\state{j}{t} + \ctrl{j}{t}\dt = \state{j}{t} + \policy{}(\state{ji}{})\dt \label{eqn:dyn2} 
\end{alignat}

A natural optimization problem may be stated as minimizing the expected time-to-goal under dynamic constraints (\ref{eqn:dyn1}, \ref{eqn:dyn2}) and collision avoidance constraint (\ref{eqn:caconstraint}).
\begin{alignat}{3}
    && \argmin\limits_{\policy{\state{}{}}} & \expectation \left[t_g \, | \ \state{}{}, \policy{\state{}{}}  \right] \\
    && s.t. \quad & \norm{\posvec{i}(t) - \posvec{j}(t)}_2 \geq \radius{i} + \radius{j} \label{eqn:caconstraint}
\end{alignat}

Model based methods can provide guarantees for solving this optimization problem, but are often assumption heavy. Reinforcement Learning  and other learning based methods may allow for more flexible, often more nonlinear policies. 
Our approach will combine a model based approach phase and a reinforcement learning phase.
The reward function will be composed of a combination of incentives for reaching the goal and penalties for close approaches and collisions with other agents, such that $\Reward{}(\state{ij}{},\ctrl{i}{})=\reward{g}+\reward{prox}+\reward{coll}$. The values of the reward function will follow the \cite{chen2017cadrl}, and an alternate reward formulation will be presented in following sections.
\subsection{Training Process}
The first process of training is a variant of inverse Reinforcement Learning, where generated ORCA trajectories are used as "expert" demonstrations. The output of this step will be a trained neural network imitating a value function for collision avoidance policies. The optimal value function is defined below.
\begin{equation}
\Value(\state{ij}{})=\sum_{t=0}^T \discount^{t\cdot\vel{pref}}\Reward{}(\state{ij}{t}, \, \policy{}^{\star}(\state{ij}{t}))
\end{equation}


The neural network approximation of the optimal value function can then be used for retrieving the optimal policy of an agent.
\begin{alignat}{3}
    &&&\policy{}^{\star}(\state{ij}{t+1})=\argmax\limits_{\ctrl{}{}} \Reward{}(\state{i}{t}, \, \ctrl{i}{}) + \label{eqn:lookahead}\\
    &&&\qquad\gamma^{\dt \cdot \vel{pref}} \int_{\state{ij}{t+1}} \transition{\state{ij}{t}}{\state{ij}{t+1}}{\ctrl{i}{}} \Value(\state{ij}{t+1}) \, d\state{ij}{t+1}\nonumber
\end{alignat}

The next phase of training is reinforcement learning in an attempt to improve the value function learned from the ORCA demonstrations and incorporate reward functions. During this phase, trajectories are rolled out using $\epsilon$-greedy exploration, while training the value function to learn and improve the policy. 

\subsubsection{Rotational invariance}
\noindent Because the optimal policy should be invariant to any coordinate transformation (rotation and translation), there is some redundancy in the rotated joint state $\state{ij'}{}$ that is provided as an input to the learned value network. Note that $(\cdot)'$ indicates a rotated quantity and that $d_g$ and $d_{ij}$ represent the distance from Robot i to goal and Robot j, respectively.
\begin{alignat}{3}
    &\state{ij}{} & \ = \ &\texttt{rotate}(\state{ij}{})\\
    && \ = \ & (d_{i,g}, \, \vel{pref}, \, \vel{i,x}', \, \vel{i,y}', \, r_i, \, \headangle{i}', \, \vel{j,x}', \, \vel{j,y}', \nonumber \\
    && \ & \pos{j,x}', \, \pos{j,y}', \, r_j, \, r_i + r_j, \, \cos{(\headangle{i}')}, \, \sin{(\headangle{i}')}, \, d_{ij} ) \nonumber
\end{alignat}
In order to get remove rotational ambiguity, an agent-centric frame is defined, with the origin at the agent’s position, and the x-axis pointing toward the agent’s goal, as shown in Fig. \ref{fig:rot_state}.
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{docs/latex/figures/rotated_state.png}
    \caption{Visualization of the Rotated State.}
    \label{fig:rot_state}
\end{figure}

As angles are periodic, we constrain our $\theta$ to be between $-\pi$ and $\pi$ before feeding it to the NN, something not done in \cite{chen2017cadrl}. As $V(\theta)$ should be equal to $V(\theta+2\pi)$, we assume this can only improve the algorithm. 



\subsection{Proposed experiments}
The resulting policy is evaluated in unstructured environments (no modelled obstacles) with varying degrees of change from the original application.\\

\begin{comment}
An experiment we wish to run is to put our agent with an agent that has another policy, so that the reciprocity hypothesis does not stand. Other robots may become stationary obstacles or behave in a non-cooperative manner, part of the robot state may become corrupted with error, the assumed dynamics model undergoes a slight change. The goal is to perturb the expected conditions for the robots and evaluate the method under these adversarial conditions.\\

One parameter we want to change is $\discount{}$, the discount factor. Currently $\discount{}=0.8$, and we would like to lower it to see the influence it has on metrics such as the time to goal. The lower $\discount{}$ is, the less importance is put on further states, and we do not know how would that translate in terms of time to goal.\\
\end{comment}
A function we wish to modify is the reward function. This function awards
the agent for reaching its goal, and penalizes the agent for
getting too close or colliding with the other agent.
Currently, the reward function is :
\begin{equation} 
\Reward{}(\state{ij}{}, \, \textbf{a})=
\left\{
    \begin{array}{ll}
        -0.25 & \mbox{if } d_{min}<0  \\
        -0.1 - d_{min}/2 & \mbox{else if } d_{min}<0.2\\
        1 & \mbox{else if }  \textbf{p}=p_{g} \\
        0 & \mbox{otherwise }\\
    \end{array}
\right. 
\label{eqn:reward1}
\end{equation}


where $d_{min}$ is the minimum separation distance between
the two agents within a duration of $\Delta t$,
We will change the penalty and reward values, as well as the value of $d_{min}$.\\
The updated reward function is given below
\begin{equation} 
\Reward{}(\state{ij}{}, \, \textbf{a})=
\left\{
    \begin{array}{ll}
        -0.75 & \mbox{if } d_{min}<0  \\
        -0.5 - d_{min}/2 & \mbox{else if } d_{min}<0.5\\
        1 & \mbox{else if }  \textbf{p}=p_{g} \\
        0 & \mbox{otherwise }\\
    \end{array}
\right. 
\label{eqn:reward2}
\end{equation}
These reward functions will be later referred to as "reward function 0" and "reward function 1". 
As in the original algorithm, cooperation is encouraged by adding a penalty term based on a comparison of the two agents’ extra time to reach the goal,
\begin{equation} 
t_{e} = t_{g}- \frac{d_{g}}{v_{pref}} 
\end{equation}
If $t_{e} < e_{l}$ and $t_{e} > e_{u}$, one agent reached its goal quickly, while the other agent took a long time. In this case a penalty of 0.1 will be subtracted from
the training value. 

\subsection{Evaluation}

In order to evaluate the inverse reinforcement learning step, the value function is visualized through a series of heatmaps. The value function is evaluated on its interpretability. 

In order to evaluate our policy, throughout training we pick initial states and propagate our state greedily using one step lookahead with our reward and value function. We do this until we have 10 successful runs and look at the total number of trajectories that are

\begin{itemize}
    \item Collision-leading
    \item Slow to reach goal: $>25$ timesteps
\end{itemize}
We evaluate over 100 trajectories at the end. 

We also do a qualitative analysis of the trajectories, which involves plotting trajectories and analyzing the smoothness of the resulting trajectories.
\section{Implementation}

\subsection{Value Network Supervised Learning}
The value network was first trained using supervised learning (SL) with example ORCA trajectories. To encourage interaction in training, ORCA trajectories were generated with two robots starting opposite halves of the plane, with their corresponding goals opposite to them. Figure \ref{fig:training_init} shows this distribution of initial states and goals. As seen in the figure, a buffer was placed in the middle of the plane to ensure that the robots did not start in an overlapping configuration. ORCA trajectories were generated for 1000 random sets of initial states and goals. Note that some trajectories still may not require a collision-avoidance technique (e.g. if the robots do not cross each others' path en route to goal).

The value network is not trained on full trajectories, rather, each example is a rotated joint state and relies on order (i.e. $\state{12}{}\neq\state{21}{}$). Each trajectory is a maximum of 100 time steps long, leading to an upper bound of $(1000 \text{ simulations})\times(100\text{ time steps})\times(2 \text{ robots}) \, = \ (200,000 \text{ training examples})$, but since some trajectories take fewer than 100 time steps to reach goal, the true number of training examples is $\approx108,000$.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{docs/latex/figures/training_data_distribution.png}
    \caption{SL Training Data Initial State and Goal Distribution - 1000 Simulations}
    \label{fig:training_init}
\end{figure}

% \begin{figure}[h!]
%     \vspace{-0.2in}
%     \centering
%     \includegraphics[width=0.4\textwidth]{docs/latex/figures/train_goals_distribution.png}
%     \caption{SL Training Data Initial Goal Distribution - 1000 Simulations}
% \end{figure}

The purpose of the training process is to encode a function that predicts $\discount{}^{t_g\cdot\vel{pref}}$ given $\state{ij}{}$, so the truth value for each state given an ORCA trajectory is computed as $y(\state{ij}{t}, \, t)=\discount{}^{(t_{f}-t)\cdot\vel{pref}}$ for each step along a trajectory. An example ORCA trajectory from the training dataset is shown in Fig. \ref{fig:ORCA}.

This step can be viewed as an inverse reinforcement learning step, a value network is being trained which will eventually influence the selection of robot actions.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{docs/latex/figures/ORCA_Example_1.png}
    \caption{ORCA Trajectory Example}
    \label{fig:ORCA}
\end{figure}

\subsection{Value Network Reinforcement Learning}
The value network was improved via reinforcement learning by selecting random episodes from the training dataset and using $\epsilon$-greedy exploration to generate trajectories for the two robots moving past each other. This process involves selecting a random action for each robot at each timestep with probability $\epsilon$ and using a one-step lookahead (Eq.~\ref{eqn:lookahead}) to select the best available action otherwise. The $\epsilon$ probability parameter was decayed linearly from $0.5$ to $0.1$ over all RL training episodes. To expedite the lookahead process and emulate a kinematically-constrained robot, the action space was limited to a subset of 25 evenly-spaced actions near the current heading direction and 25 random actions also in this neighborhood. The restricted action space is shown in Fig. \ref{fig:actionspace}, where the $\headangle{}\pm\frac{\pi}{6}$ cone and 5 velocity magnitude scales can be seen for an agent at the origin with $\vel{pref}=1$.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{docs/latex/figures/action_space.PNG}
    \caption{Action Space for Lookahead}
    \label{fig:actionspace}
\end{figure}

Given the simulated trajectories using the value function, the successful (collision-avoiding and goal-reaching) trajectories were used to create state-value pairs (labels for training the network). To do this, the rewards during the reinforcement learning simulation were used in a greedy one-step lookahead function with the current value model. The resulting state-value pairs were assimilated into the dataset, meaning the corresponding ORCA trajectories were replaced with the simulated trajectories. The effect was to include randomness and exploration of the state space early in the training process. As training continued, the dataset became more dominated by simulated trajectories from the trained value function, which would help the network converge to an optimal solution.

The action space generation and reward function implementations were vectorized in order to reduce computational burden during training.

\section{Results}

The value function attained via SL using example ORCA trajectories can be visualized in a simplified manner using a heatmap in the 2D plane. By holding all variables constant except for the position of one of the robots, the output of the value function can be mapped to Cartesian coordinates. This is exemplified in Fig. \ref{fig:heatmap_rot}, where the position and velocity of Robot 1 are held constant while Robot 2 is moved around the plane. These two heatmaps show that the value function learned a rotation invariance, as we attain nearly the same value distribution in both Fig. \ref{fig:heatmap_x} and  Fig. \ref{fig:heatmap_y} with a simple $90\degree$ rotation. We may also visualize the resulting value distribution by varying Robot 1's state while holding Robot 2 constant. In Fig. \ref{fig:heatmap_2}, Robot 2 is stationary in the plane, and Robot 1's position is varied while maintaining a velocity vector in the direction of goal. We can compare the SL policy to the RL policy be observing the difference in Fig. \ref{fig:heatmap_21} and Fig. \ref{fig:heatmap_22}. In both cases, however, the trend is the same, the value network predicts that the time-to-goal will be low when Robot 1 is near it's goal, and the time-to-goal will be high when Robot 1 is far from it's goal or impeded by Robot 2. Improvement can be seen in this aspect after RL value network improvement.
In fact the contrasts are much higher in the post RL heatmap, indicating that the network is more confident. 
\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{docs/latex/figures/heatmap_x_Robot_2.png}
         \caption{Original Orientation.}
         \label{fig:heatmap_x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{docs/latex/figures/heatmap_y_Robot_2.png}
         \caption{Rotated Orientation.}
         \label{fig:heatmap_y}
     \end{subfigure}
     \caption{Example of Rotation Invariance of Supervised Learning Value Network Output.}\label{fig:heatmap_rot}
\end{figure*}

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{docs/latex/figures/heatmap_1_Robot_1.png}
         \caption{SL Value Heatmap Varying Robot 1.}
         \label{fig:heatmap_21}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{docs/latex/figures/heatmap_rl_robot1.png}
         \caption{RL Value Heatmap Varying Robot 1.}
         \label{fig:heatmap_22}
     \end{subfigure}
     \caption{Comparison of Supervised- and Reinforcement-Learning Value Networks.}\label{fig:heatmap_2}
\end{figure*}

We compare results from our two reward functions. Figures \ref{fig:rew_0_coll} and \ref{fig:rew_1_coll} show that collisions decrease rapidly in both cases and then decrease rather slowly, but steadily. It can be seen in these figures that there is a tradeoff between the number of colliding runs and the number of successful (or time-efficient) runs. Figure \ref{fig:rew_1_coll}, showing performance given by the reward function from Eqn. \ref{eqn:reward2}, shows that with a more aggressive collision-avoiding strategy, the robots collide less often but also don't reach their goals within the allotted time as successfully. This figure proves the algorithm is finding more value in avoiding trajectories to the extent of potentially being unable to reach the goal. These results suggest that it would be prudent to use the less conservative reward function 0.

At the end of training we have these results for our two reward functions and our initial value function. 
\begin{center}
\begin{tabular}{||c c c c c||} 
 \hline
  & \#Attempts & \#Collision  & \#Failure  & \#Success \\ [0.5ex] 
 \hline\hline
 Pre RL & 100 & 84 & 8  & 8 \\ 
 \hline
 Rew. 0 & 100 & 31 & 6  & 63 \\
 Rew. 1 & 100 & X & X  & X \\
 \hline
\end{tabular}
\end{center}

We see a clear improvement from using the supervised learned value function directly for both reward functions. 
\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{docs/latex/figures/statistics_rew0.png}
    \caption{Reward function 0: Collisions, Failures vs. Time}
    \label{fig:rew_0_coll}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/collisions_1.png}
    \caption{Reward function 1: Collisions/failures vs. time}
    \label{fig:rew_1_coll}
\end{figure}
\todo[inline]{include plots showing collisions going down}

Finally, we show a propagated trajectory from three different value functions: The value function outputted from the ORCA imitation learning \ref{fig:initial_traj}, the value function from the RL with the original reward function \ref{fig:rew_0_traj} and the value function from our updated reward function \ref{fig:rew_1_traj}. Firstly, the RL trajectories both show collision avoiding trajectories in comparison to the pre-RL trajectory. Further, the agent trained on the updated reward function is significantly more collision avoiding than the other post-RL agent. This is clearly an effect of higher penalty in the reward. 

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/traj_0.png}
    \caption{Reward function 0: Sample Trajectory}
    \label{fig:rew_0_traj}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/traj_1.png}
    \caption{Reward function 1: Sample Trajectory}
    \label{fig:rew_1_traj}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/initial_traj.png}
    \caption{Initial ORCA value function: Sample Trajectory}
    \label{fig:initial_traj}
\end{figure}
\section{Conclusion}
In this paper, we have adapted the method from Chen et. al. \cite{chen2017cadrl}, modifying several parameters and functions. The results are promising showing that model based collision-avoiding behavior can be influenced and controlled by reinforcement learning methods and reward tuning. One other advantage of the RL based approach is that it does not require knowing the goal states or preferred velocities of other robots. This partially observable non-communicative scenario is more realistic in real life. However, the RL approach loses the guarantees of the model-based approaches.  

Further testing on different scenarios and multi-agent systems must be done to verify our results. 

For future research one could also change the behaviors of the other robots and observe the effects on training and evaluation. In addition, we noticed that some states may be redundant in the input of the neural network (e.g, $ r_j $,$ r_i $, $r_j+r_i$) and we should study the effect of removing the redundancy. Researching changes in reward/cooperation parameters and also tuning hyperparameters of the neural network/reinforcement learning algorithm are low-hanging fruit for other interesting projects. 

\bibliographystyle{IEEEtran.bst}
\bibliography{references.bib}
\end{document}
